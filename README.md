# Reading List

This repository collates the papers I am reading for my MSCS thesis.

### 2015 -- present

|Paper|Method|Venue|Code|
|-----|------|-----|----|
|[Analyzing and Improving Representations with the Soft Nearest Neighbor Loss](https://arxiv.org/abs/1902.01889)|SNNL|ICML 2019||
|[Variational Mixture-of-Experts Autoencoders for Multi-Modal Deep Generative Models](https://arxiv.org/abs/1911.03393)|MMVAE|NeurIPS 2019|[PyTorch](https://github.com/iffsid/mmvae)|
|[Multimodal Generative Models for Scalable Weakly-Supervised Learning](https://arxiv.org/abs/1802.05335)|MVAE|NeurIPS 2018|[PyTorch](https://github.com/mhw32/multimodal-vae-public)|
|[Cooperative Learning of Audio and Video Models from Self-Supervised Synchronization](https://arxiv.org/abs/1807.00230)|SS AVTS|NeurIPS 2018||
|[Cooperative neural networks: Exploiting prior independence structure for improved classification](https://arxiv.org/abs/1906.00291)|CoNN|NeurIPS 2018|[PyTorch](https://github.com/Harshs27/CoNN)|
|[Learning to Specialize with Knowledge Distillation for Visual Question Answering](https://papers.nips.cc/paper/2018/hash/0f2818101a7ac4b96ceeba38de4b934c-Abstract.html)|MCL-KD|NeurIPS 2018|[PyTorch](https://github.com/JonghwanMun/MCL-KD)|
|[KATE: K-Competitive Autoencoder for Text](https://arxiv.org/abs/1705.02033)|KATE|KDD 2017|[Keras](https://github.com/hugochan/KATE)|
|[Distilling the Knowledge in Neural Networks](https://arxiv.org/abs/1503.02531)|KD|NeurIPS 2015 Workshop||
|[A network of deep neural networks for distant speech recognition](https://arxiv.org/abs/1703.08002)|CN-DNN|ICASSP 2017||
|[Cooperative Training for Generative Modeling of Discrete Data](https://arxiv.org/abs/1804.03782)|CoT|ICML 2019||
|[Learning a Text-Video Embedding from Incomplete and Heterogeneous Data](https://arxiv.org/abs/1804.02516)|MEE||[PyTorch](https://github.com/antoine77340/Mixture-of-Embedding-Experts)|
|[MIX'EM: Unsupervised Image Classification using a Mixture of Embeddings](https://arxiv.org/abs/2007.09502)|MIX'EM|ACCV 2020||


### 2015 and older

|Paper|Method|Venue|Code|
|-----|------|-----|----|

